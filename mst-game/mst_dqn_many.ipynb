{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"DQN agents trained on Minimum Spanning Tree by independent Q-learning.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import dqn\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "import mst_setup as mst\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Training parameters\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"/tmp/dqn_test\",\n",
    "                    \"Directory to save/load the agent.\")\n",
    "flags.DEFINE_integer(\"num_train_episodes\", int(1e6),\n",
    "                     \"Number of training episodes.\")\n",
    "flags.DEFINE_integer(\n",
    "    \"eval_every\", 10000,\n",
    "    \"Episode frequency at which the DQN agents are evaluated.\")\n",
    "\n",
    "# DQN model hyper-parameters\n",
    "flags.DEFINE_string(\"game\", \"mst\", \"Name of the game\")\n",
    "\n",
    "flags.DEFINE_list(\"hidden_layers_sizes\", [64, 64],\n",
    "                  \"Number of hidden units in the Q-Network MLP.\")\n",
    "flags.DEFINE_integer(\"replay_buffer_capacity\", int(1e5),\n",
    "                     \"Size of the replay buffer.\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32,\n",
    "                     \"Number of transitions to sample at each learning step.\")\n",
    "flags.DEFINE_integer(\"num_nodes\", 10, \"Number of Nodes to Train On\")\n",
    "\n",
    "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n",
    "  \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = random_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = env.reset()\n",
    "      episode_rewards = 0\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        if env.is_turn_based:\n",
    "          agent_output = cur_agents[player_id].step(\n",
    "              time_step, is_evaluation=True)\n",
    "          action_list = [agent_output.action]\n",
    "        else:\n",
    "          agents_output = [\n",
    "              agent.step(time_step, is_evaluation=True) for agent in cur_agents\n",
    "          ]\n",
    "          action_list = [agent_output.action for agent_output in agents_output]\n",
    "        time_step = env.step(action_list)\n",
    "        episode_rewards += time_step.rewards[player_pos]\n",
    "      sum_episode_rewards[player_pos] += episode_rewards\n",
    "  return sum_episode_rewards / num_episodes\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  game = FLAGS.game # Set the game\n",
    "  num_players = 1\n",
    "  games, rewards = mst.game_params(1000,#FLAGS.num_train_episodes, \n",
    "                                    FLAGS.num_nodes) # Need to load games in from file and work with them\n",
    "  env_configs = games[0]\n",
    "  env = rl_environment.Environment(game, **env_configs)\n",
    "  info_state_size = FLAGS.num_nodes * FLAGS.num_nodes * 3 #env.observation_spec()[\"info_state\"][0]\n",
    "  num_actions = env.action_spec()[\"num_actions\"] # number of possible actions\n",
    "\n",
    "  print(\"Info State Size: \", info_state_size)\n",
    "  print(\"Num Actions: \", num_actions)  \n",
    "    \n",
    "  # random agents for evaluation\n",
    "  random_agents = [\n",
    "      random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "      for idx in range(num_players)\n",
    "  ]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('./dqn_checkpoints/dqn_20epochs_mst_easy/dqn_test-689999.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('/tmp/'))\n",
    "    hidden_layers_sizes = [int(l) for l in FLAGS.hidden_layers_sizes]\n",
    "    # pylint: disable=g-complex-comprehension\n",
    "    agents = [\n",
    "        dqn.DQN(\n",
    "            session=sess,\n",
    "            player_id=idx,\n",
    "            state_representation_size=info_state_size,\n",
    "            num_actions=num_actions,\n",
    "            hidden_layers_sizes=hidden_layers_sizes,\n",
    "            replay_buffer_capacity=FLAGS.replay_buffer_capacity,\n",
    "            batch_size=FLAGS.batch_size) for idx in range(num_players)\n",
    "    ]\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for ep in range(FLAGS.num_train_episodes):\n",
    "      env_configs = games[ep % len(games)]\n",
    "      env = rl_environment.Environment(game, **env_configs)\n",
    "      episode_reward = rewards[ep % len(games)]\n",
    "      if (ep + 1) % FLAGS.eval_every == 0:\n",
    "        r_mean = eval_against_random_bots(env, agents, random_agents, 100)\n",
    "        logging.info(\"[%s] Mean episode rewards %s\", ep + 1, r_mean)\n",
    "        saver.save(sess, FLAGS.checkpoint_dir, ep)\n",
    "        print(\"Actual MST Value: \", episode_reward)\n",
    "\n",
    "      #env = rl_environment.Environment(game, **games[ep])\n",
    "      time_step = env.reset()\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        if env.is_turn_based:\n",
    "          agent_output = agents[player_id].step(time_step)\n",
    "          action_list = [agent_output.action]\n",
    "        else:\n",
    "          agents_output = [agent.step(time_step) for agent in agents]\n",
    "          action_list = [agent_output.action for agent_output in agents_output]\n",
    "        time_step = env.step(action_list)\n",
    "\n",
    "      # Episode is over, step all agents with final info state.\n",
    "      for agent in agents:\n",
    "        agent.step(time_step)\n",
    "\n",
    "    print(\"Actual MST: \", rewards)\n",
    "if __name__ == \"__main__\":\n",
    "  app.run(main)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
